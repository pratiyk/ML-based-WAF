{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example: Replace these with actual paths to your datasets\n",
    "csic_2010_path = 'csic_final.csv'\n",
    "pkdd_2007_path = 'ecml_final.csv'\n",
    "\n",
    "# Load datasets\n",
    "csic_2010_df = pd.read_csv(csic_2010_path)\n",
    "pkdd_2007_df = pd.read_csv(pkdd_2007_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Class', 'Method', 'URI', 'Host-Header', 'Host', 'Connection', 'Accept',\n",
      "       'Accept-Charset', 'Accept-Language', 'Cache-control', 'Cookie',\n",
      "       'Pragma', 'User-Agent', 'Content-Length', 'Content-Type', 'POST-Data',\n",
      "       'GET-Query'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(csic_2010_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Class', 'Method', 'URI', 'Host-Header', 'Host', 'Connection', 'Accept',\n",
      "       'Accept-Charset', 'Accept-Language', 'Cache-control', 'Cookie',\n",
      "       'Pragma', 'User-Agent', 'Content-Length', 'Content-Type', 'POST-Data',\n",
      "       'GET-Query'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(pkdd_2007_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_request_columns(df):\n",
    "    \"\"\"\n",
    "    Combine relevant columns into a single HTTP request string.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset containing the HTTP request components.\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: Series with combined HTTP request strings.\n",
    "    \"\"\"\n",
    "    # Combine Method, URI, POST-Data, and GET-Query columns\n",
    "    combined_requests = df['Method'].astype(str) + ' ' + \\\n",
    "                        df['URI'].astype(str) + ' ' + \\\n",
    "                        df['POST-Data'].fillna('').astype(str) + ' ' + \\\n",
    "                        df['GET-Query'].fillna('').astype(str)\n",
    "    \n",
    "    return combined_requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "csic_2010_combined_requests = combine_request_columns(csic_2010_df)\n",
    "pkdd_2007_combined_requests = combine_request_columns(pkdd_2007_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSIC 2010 features shape: (61065, 32746)\n",
      "PKDD 2007 features shape: (23892, 435297)\n",
      "Feature names (first 10): ['0' '00041295x' '0004533796646501' '0007950776968836' '00098872h' '001'\n",
      " '00101492q' '0010509413049118' '00151522k' '0017018534732457']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "def tokenize_and_extract_features_from_combined(df, combined_requests):\n",
    "    \"\"\"\n",
    "    Tokenize and extract features from combined HTTP request strings.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The original dataset.\n",
    "        combined_requests (pd.Series): The combined HTTP request strings.\n",
    "        \n",
    "    Returns:\n",
    "        sparse_matrix, feature_names: Sparse matrix of features and list of feature names.\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "    X = vectorizer.fit_transform(combined_requests)\n",
    "    \n",
    "    # Don't convert to dense, just return the sparse matrix and feature names\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    return X, feature_names\n",
    "\n",
    "# Apply to each dataset\n",
    "csic_2010_features, csic_2010_feature_names = tokenize_and_extract_features_from_combined(csic_2010_df, csic_2010_combined_requests)\n",
    "pkdd_2007_features, pkdd_2007_feature_names = tokenize_and_extract_features_from_combined(pkdd_2007_df, pkdd_2007_combined_requests)\n",
    "\n",
    "# Display sparse matrix shape and feature names for debugging\n",
    "print(\"CSIC 2010 features shape:\", csic_2010_features.shape)\n",
    "print(\"PKDD 2007 features shape:\", pkdd_2007_features.shape)\n",
    "print(\"Feature names (first 10):\", csic_2010_feature_names[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSIC 2010 features shape: (61065, 10000)\n",
      "PKDD 2007 features shape: (23892, 10000)\n",
      "Feature names (first 10): ['0' '0010509413049118' '0034994508490764' '0040408141488994' '00423300p'\n",
      " '0070151205403865' '00776957v' '0088492463086794' '0093197595343977'\n",
      " '01039']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_extract_features_with_limit(df, combined_requests, max_features=10000):\n",
    "    \"\"\"\n",
    "    Tokenize and extract features with a limit on the number of features.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The original dataset.\n",
    "        combined_requests (pd.Series): The combined HTTP request strings.\n",
    "        max_features (int): Maximum number of features to consider.\n",
    "        \n",
    "    Returns:\n",
    "        sparse_matrix, feature_names: Sparse matrix of features and list of feature names.\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b', max_features=max_features)\n",
    "    X = vectorizer.fit_transform(combined_requests)\n",
    "    \n",
    "    # Return sparse matrix and feature names\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    return X, feature_names\n",
    "\n",
    "# Apply with reduced vocabulary size\n",
    "csic_2010_features, csic_2010_feature_names = tokenize_and_extract_features_with_limit(csic_2010_df, csic_2010_combined_requests)\n",
    "pkdd_2007_features, pkdd_2007_feature_names = tokenize_and_extract_features_with_limit(pkdd_2007_df, pkdd_2007_combined_requests)\n",
    "\n",
    "# Display sparse matrix shape and feature names for debugging\n",
    "print(\"CSIC 2010 features shape:\", csic_2010_features.shape)\n",
    "print(\"PKDD 2007 features shape:\", pkdd_2007_features.shape)\n",
    "print(\"Feature names (first 10):\", csic_2010_feature_names[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_request_columns(df):\n",
    "    \"\"\"\n",
    "    Combine relevant columns into a single HTTP request string.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset containing the HTTP request components.\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: Series with combined HTTP request strings.\n",
    "    \"\"\"\n",
    "    # Combine Method, URI, POST-Data, and GET-Query columns\n",
    "    combined_requests = df['Method'].astype(str) + ' ' + \\\n",
    "                        df['URI'].astype(str) + ' ' + \\\n",
    "                        df['POST-Data'].fillna('').astype(str) + ' ' + \\\n",
    "                        df['GET-Query'].fillna('').astype(str)\n",
    "    \n",
    "    return combined_requests\n",
    "\n",
    "# Example usage\n",
    "csic_2010_combined_requests = combine_request_columns(csic_2010_df)\n",
    "pkdd_2007_combined_requests = combine_request_columns(pkdd_2007_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features in matrix: 32746\n",
      "Length of feature names: 10000\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Mismatch in feature count and feature names length.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of feature names: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(csic_2010_feature_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Ensure lengths match before proceeding\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m csic_2010_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(csic_2010_feature_names), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatch in feature count and feature names length.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Proceed with the calculation\u001b[39;00m\n\u001b[0;32m     31\u001b[0m csic_2010_info_gain \u001b[38;5;241m=\u001b[39m calculate_information_gain(csic_2010_features, csic_2010_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m'\u001b[39m], csic_2010_feature_names)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Mismatch in feature count and feature names length."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_information_gain(features, target, feature_names):\n",
    "    \"\"\"\n",
    "    Calculate the information gain (mutual information) for each feature.\n",
    "    \n",
    "    Args:\n",
    "        features (scipy.sparse matrix): The feature matrix.\n",
    "        target (pd.Series): The target variable (Class labels).\n",
    "        feature_names (list): List of feature names corresponding to the columns in the feature matrix.\n",
    "        \n",
    "    Returns:\n",
    "        pd.Series: A series with feature names as the index and their corresponding information gain.\n",
    "    \"\"\"\n",
    "    # Calculate mutual information\n",
    "    info_gain = mutual_info_classif(features, target, discrete_features=True)\n",
    "    \n",
    "    # Return the information gain with the feature names as the index\n",
    "    return pd.Series(info_gain, index=feature_names)\n",
    "\n",
    "\n",
    "# Before calculating mutual information, let's check the lengths\n",
    "print(f\"Number of features in matrix: {csic_2010_features.shape[1]}\")\n",
    "print(f\"Length of feature names: {len(csic_2010_feature_names)}\")\n",
    "\n",
    "# Ensure lengths match before proceeding\n",
    "assert csic_2010_features.shape[1] == len(csic_2010_feature_names), \"Mismatch in feature count and feature names length.\"\n",
    "\n",
    "# Proceed with the calculation\n",
    "csic_2010_info_gain = calculate_information_gain(csic_2010_features, csic_2010_df['Class'], csic_2010_feature_names)\n",
    "pkdd_2007_info_gain = calculate_information_gain(pkdd_2007_features, pkdd_2007_df['Class'], pkdd_2007_feature_names)\n",
    "\n",
    "# Example usage with your dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 CSIC 2010 features by information gain:\n",
      "imagenes    0.048587\n",
      "b1          0.030431\n",
      "tienda1     0.029950\n",
      "27          0.029316\n",
      "publico     0.028146\n",
      "2f          0.026662\n",
      "3d          0.025685\n",
      "jpg         0.023272\n",
      "modo        0.021713\n",
      "3b          0.021557\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you already have a vectorizer fitted to the data:\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "X = vectorizer.fit_transform(csic_2010_combined_requests)  # Assuming csic_2010_combined_requests is your combined data\n",
    "\n",
    "# Now get the correct feature names from the fitted vectorizer\n",
    "csic_2010_feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Now you can calculate information gain\n",
    "def calculate_information_gain(features, target, feature_names):\n",
    "    info_gain = mutual_info_classif(features, target, discrete_features=True)\n",
    "    return pd.Series(info_gain, index=feature_names)\n",
    "\n",
    "# Calculate information gain\n",
    "csic_2010_info_gain = calculate_information_gain(X, csic_2010_df['Class'], csic_2010_feature_names)\n",
    "\n",
    "# Display the top 10 features with the highest information gain\n",
    "print(\"Top 10 CSIC 2010 features by information gain:\")\n",
    "print(csic_2010_info_gain.sort_values(ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 CSIC 2010 features:\n",
      "Index(['imagenes', 'b1', 'tienda1', '27', 'publico', '2f', '3d', 'jpg', 'modo',\n",
      "       '3b', 'wide', 'asf', 'global', '3e', '3c', 'cookie', '29', '253a',\n",
      "       'get', 'nombre'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def select_top_features(info_gain, top_n=20):\n",
    "    \"\"\"\n",
    "    Select the top N features based on information gain.\n",
    "    \n",
    "    Args:\n",
    "        info_gain (pd.Series): Information gain values for each feature.\n",
    "        top_n (int): Number of top features to select.\n",
    "        \n",
    "    Returns:\n",
    "        list: List of top N feature names.\n",
    "    \"\"\"\n",
    "    top_features = info_gain.sort_values(ascending=False).head(top_n).index\n",
    "    return top_features\n",
    "\n",
    "# Example: Select top 20 features\n",
    "top_csic_2010_features = select_top_features(csic_2010_info_gain, top_n=20)\n",
    "print(\"Top 20 CSIC 2010 features:\")\n",
    "print(top_csic_2010_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of top CSIC 2010 features DataFrame: (61065, 20)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import pandas as pd\n",
    "\n",
    "def filter_features(features, top_feature_names, feature_names):\n",
    "    \"\"\"\n",
    "    Filter the features to include only the top feature names.\n",
    "    \n",
    "    Args:\n",
    "        features (scipy.sparse matrix): The original feature matrix.\n",
    "        top_feature_names (list): List of top feature names to retain.\n",
    "        feature_names (numpy.ndarray): Array of all feature names.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with only the top features.\n",
    "    \"\"\"\n",
    "    # Find indices of top feature names\n",
    "    indices = np.array([np.where(feature_names == name)[0][0] for name in top_feature_names])\n",
    "    # Create a new feature matrix with only the top features\n",
    "    return features[:, indices]\n",
    "\n",
    "# Example usage\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "X = vectorizer.fit_transform(csic_2010_combined_requests)  # Fit and transform with your combined requests\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Assume top_csic_2010_features is already defined\n",
    "csic_2010_top_features_matrix = filter_features(X, top_csic_2010_features, feature_names)\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "csic_2010_top_features_df = pd.DataFrame(csic_2010_top_features_matrix.toarray(), columns=top_csic_2010_features)\n",
    "print(\"Shape of top CSIC 2010 features DataFrame:\", csic_2010_top_features_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset shape: (61065, 21)\n"
     ]
    }
   ],
   "source": [
    "def combine_with_target(features_df, target):\n",
    "    \"\"\"\n",
    "    Combine features DataFrame with target variable.\n",
    "    \n",
    "    Args:\n",
    "        features_df (pd.DataFrame): DataFrame of selected features.\n",
    "        target (pd.Series): The target variable (Class labels).\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame with features and target.\n",
    "    \"\"\"\n",
    "    return pd.concat([features_df, target.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Combine top features with the target variable\n",
    "csic_2010_final_df = combine_with_target(csic_2010_top_features_df, csic_2010_df['Class'])\n",
    "print(\"Final dataset shape:\", csic_2010_final_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
