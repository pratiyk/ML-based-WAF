{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook Analyzes and Processes HTTP Requests and Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTTP Request Parser for .csv Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "def extract_parameters(param_str):\n",
    "    \"\"\"\n",
    "    Extracts parameters from a URL-encoded string and returns as a formatted string.\n",
    "    \"\"\"\n",
    "    params = parse_qs(param_str)\n",
    "    return '; '.join([f'{k}={v[0]}' for k, v in params.items()])\n",
    "\n",
    "def count_special_characters(s):\n",
    "    \"\"\"\n",
    "    Counts occurrences of special characters and patterns commonly associated with attacks.\n",
    "    \"\"\"\n",
    "    # Define patterns associated with attacks\n",
    "    attack_patterns = {\n",
    "        'single_quote': r\"'\",\n",
    "        'double_quote': r'\"',\n",
    "        'backslash': r'\\\\',\n",
    "        'semicolon': r';',\n",
    "        'double_dash': r'--',\n",
    "        'asterisk': r'\\*',\n",
    "        'hash': r'#',\n",
    "        'percent': r'%',\n",
    "        'ampersand': r'&',\n",
    "        'pipe': r'\\|',\n",
    "        'question_mark': r'\\?',\n",
    "        'equal_sign': r'=',\n",
    "        'parentheses': r'\\(|\\)',\n",
    "        'angle_brackets': r'<|>',\n",
    "        'curly_brackets': r'\\{|\\}',\n",
    "        'square_brackets': r'\\[|\\]',\n",
    "        'dollar_sign': r'\\$',\n",
    "        'at_symbol': r'@',\n",
    "        'tilde': r'~',\n",
    "        'backtick': r'`',\n",
    "        'slash': r'/',\n",
    "        'colon': r':',\n",
    "        'exclamation_mark': r'!',\n",
    "        'javascript_scheme': r'javascript:',\n",
    "        'eval_function': r'eval\\(',\n",
    "        'alert_function': r'alert\\(',\n",
    "        'file_inclusion': r'file://',\n",
    "        'path_traversal': r'\\.\\./',\n",
    "        'localhost': r'localhost',\n",
    "        'root_path': r'root'\n",
    "    }\n",
    "    \n",
    "    # Count occurrences of each pattern\n",
    "    total_count = sum(len(re.findall(pattern, s, re.IGNORECASE)) for pattern in attack_patterns.values())\n",
    "    \n",
    "    return total_count\n",
    "\n",
    "def parse_txt_file(file_in):\n",
    "    \"\"\"\n",
    "    Parses a .txt file and returns a list of log entries.\n",
    "    \"\"\"\n",
    "    with open(file_in, 'r', encoding='utf-8') as fin:\n",
    "        return fin.readlines()\n",
    "\n",
    "def parse_file(file_in, file_out):\n",
    "    # Determine file type\n",
    "    if file_in.endswith('.txt'):\n",
    "        lines = parse_txt_file(file_in)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Only .txt and .xml files are supported.\")\n",
    "\n",
    "    with open(file_out, 'w', newline='', encoding='utf-8') as fout:\n",
    "        csv_writer = csv.writer(fout)\n",
    "        # Write the header\n",
    "        csv_writer.writerow([\n",
    "            'Method', 'Full URL', 'URL Path', 'Query Params', 'Query Params Length', 'Number of Query Params', 'Body Params', \n",
    "            'Content-Length', 'Content-Type', 'User-Agent', 'Host', 'Accept', \n",
    "            'Accept-Encoding', 'Accept-Charset', 'Accept-Language', 'Pragma',\n",
    "            'Connection', 'Body Length', 'URL Length', 'Special Characters Count in URL', 'Special Characters Count in Query Params'\n",
    "        ])\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i].strip()\n",
    "            if line.startswith(('GET', 'POST', 'PUT', 'DELETE', 'HEAD', 'TRACE', 'PATCH', 'OPTIONS')):\n",
    "                # Extract Method\n",
    "                method = line.split(' ')[0]\n",
    "                \n",
    "                # Extract Full URL\n",
    "                full_url = line.split(' ')[1]\n",
    "                \n",
    "                # Extract URL Path\n",
    "                url_path = urlparse(full_url).path\n",
    "                \n",
    "                # Extract Query Parameters\n",
    "                query_params = parse_qs(urlparse(full_url).query)\n",
    "                query_params_str = extract_parameters(urlparse(full_url).query)\n",
    "                query_params_length = len(urlparse(full_url).query)\n",
    "                num_query_params = len(query_params)\n",
    "                \n",
    "                # Count special characters\n",
    "                url_special_characters_count = count_special_characters(url_path)\n",
    "                query_special_characters_count = count_special_characters(urlparse(full_url).query)\n",
    "                \n",
    "                # Initialize other fields\n",
    "                content_length = ''\n",
    "                content_type = ''\n",
    "                user_agent = ''\n",
    "                host = ''\n",
    "                accept = ''\n",
    "                accept_encoding = ''\n",
    "                accept_charset = ''\n",
    "                accept_language = ''\n",
    "                pragma = ''\n",
    "                connection = ''\n",
    "                body_params_str = ''\n",
    "                \n",
    "                headers = {}\n",
    "                body = ''\n",
    "                \n",
    "                # Loop through headers and body\n",
    "                j = 1\n",
    "                while i + j < len(lines) and not lines[i + j].strip() == '':\n",
    "                    header_line = lines[i + j].strip()\n",
    "                    if header_line.startswith('Content-Length:'):\n",
    "                        content_length = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Content-Type:'):\n",
    "                        content_type = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('User-Agent:'):\n",
    "                        user_agent = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Host:'):\n",
    "                        host = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept:'):\n",
    "                        accept = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Encoding:'):\n",
    "                        accept_encoding = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Charset:'):\n",
    "                        accept_charset = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Language:'):\n",
    "                        accept_language = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Pragma:'):\n",
    "                        pragma = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Connection:'):\n",
    "                        connection = header_line.split(':', 1)[1].strip()\n",
    "                    j += 1\n",
    "                \n",
    "                # The body is the line after the headers\n",
    "                if i + j < len(lines):\n",
    "                    body = lines[i + j].strip()\n",
    "                    # Extract body parameters (if the body is URL-encoded)\n",
    "                    body_params_str = extract_parameters(body)\n",
    "                \n",
    "                # Calculate lengths for additional features\n",
    "                body_length = len(body)\n",
    "                url_length = len(full_url)\n",
    "                    \n",
    "                # Append the extracted features to the CSV\n",
    "                csv_writer.writerow([\n",
    "                    method, full_url, url_path, query_params_str, query_params_length, num_query_params, body_params_str, \n",
    "                    content_length, content_type, user_agent, host, accept, \n",
    "                    accept_encoding, accept_charset, accept_language, pragma,\n",
    "                    connection, body_length, url_length,\n",
    "                    url_special_characters_count, query_special_characters_count\n",
    "                ])\n",
    "                \n",
    "                i += j + 1\n",
    "            else:\n",
    "                i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file('dataset/CSIC 2010/anomalousTrafficTest.txt', 'anomalous_parsed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file('dataset/CSIC 2010/normalTrafficTraining.txt', 'normal_parsed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analysis of Parsed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Full URL</th>\n",
       "      <th>URL Path</th>\n",
       "      <th>Query Params</th>\n",
       "      <th>Query Params Length</th>\n",
       "      <th>Number of Query Params</th>\n",
       "      <th>Body Params</th>\n",
       "      <th>Content-Length</th>\n",
       "      <th>Content-Type</th>\n",
       "      <th>User-Agent</th>\n",
       "      <th>...</th>\n",
       "      <th>Accept</th>\n",
       "      <th>Accept-Encoding</th>\n",
       "      <th>Accept-Charset</th>\n",
       "      <th>Accept-Language</th>\n",
       "      <th>Pragma</th>\n",
       "      <th>Connection</th>\n",
       "      <th>Body Length</th>\n",
       "      <th>URL Length</th>\n",
       "      <th>Special Characters Count in URL</th>\n",
       "      <th>Special Characters Count in Query Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/tienda1/publico/anadir.j...</td>\n",
       "      <td>/tienda1/publico/anadir.jsp</td>\n",
       "      <td>id=2; nombre=Jam�n Ib�rico; precio=85; cantida...</td>\n",
       "      <td>146</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>...</td>\n",
       "      <td>text/xml,application/xml,application/xhtml+xml...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>close</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POST</td>\n",
       "      <td>http://localhost:8080/tienda1/publico/anadir.jsp</td>\n",
       "      <td>/tienda1/publico/anadir.jsp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>146.0</td>\n",
       "      <td>application/x-www-form-urlencoded</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>...</td>\n",
       "      <td>text/xml,application/xml,application/xhtml+xml...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>close</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/tienda1/publico/anadir.j...</td>\n",
       "      <td>/tienda1/publico/anadir.jsp</td>\n",
       "      <td>id=2/; nombre=Jam�n Ib�rico; precio=85; cantid...</td>\n",
       "      <td>77</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>...</td>\n",
       "      <td>text/xml,application/xml,application/xhtml+xml...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>close</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POST</td>\n",
       "      <td>http://localhost:8080/tienda1/publico/anadir.jsp</td>\n",
       "      <td>/tienda1/publico/anadir.jsp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.0</td>\n",
       "      <td>application/x-www-form-urlencoded</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>...</td>\n",
       "      <td>text/xml,application/xml,application/xhtml+xml...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>close</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/asf-logo-wide.gif~</td>\n",
       "      <td>/asf-logo-wide.gif~</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>...</td>\n",
       "      <td>text/xml,application/xml,application/xhtml+xml...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>close</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Method                                           Full URL  \\\n",
       "0    GET  http://localhost:8080/tienda1/publico/anadir.j...   \n",
       "1   POST   http://localhost:8080/tienda1/publico/anadir.jsp   \n",
       "2    GET  http://localhost:8080/tienda1/publico/anadir.j...   \n",
       "3   POST   http://localhost:8080/tienda1/publico/anadir.jsp   \n",
       "4    GET           http://localhost:8080/asf-logo-wide.gif~   \n",
       "\n",
       "                      URL Path  \\\n",
       "0  /tienda1/publico/anadir.jsp   \n",
       "1  /tienda1/publico/anadir.jsp   \n",
       "2  /tienda1/publico/anadir.jsp   \n",
       "3  /tienda1/publico/anadir.jsp   \n",
       "4          /asf-logo-wide.gif~   \n",
       "\n",
       "                                        Query Params  Query Params Length  \\\n",
       "0  id=2; nombre=Jam�n Ib�rico; precio=85; cantida...                  146   \n",
       "1                                                NaN                    0   \n",
       "2  id=2/; nombre=Jam�n Ib�rico; precio=85; cantid...                   77   \n",
       "3                                                NaN                    0   \n",
       "4                                                NaN                    0   \n",
       "\n",
       "   Number of Query Params  Body Params  Content-Length  \\\n",
       "0                       5          NaN             NaN   \n",
       "1                       0          NaN           146.0   \n",
       "2                       5          NaN             NaN   \n",
       "3                       0          NaN            77.0   \n",
       "4                       0          NaN             NaN   \n",
       "\n",
       "                        Content-Type  \\\n",
       "0                                NaN   \n",
       "1  application/x-www-form-urlencoded   \n",
       "2                                NaN   \n",
       "3  application/x-www-form-urlencoded   \n",
       "4                                NaN   \n",
       "\n",
       "                                          User-Agent  ...  \\\n",
       "0  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  ...   \n",
       "1  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  ...   \n",
       "2  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  ...   \n",
       "3  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  ...   \n",
       "4  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  ...   \n",
       "\n",
       "                                              Accept  \\\n",
       "0  text/xml,application/xml,application/xhtml+xml...   \n",
       "1  text/xml,application/xml,application/xhtml+xml...   \n",
       "2  text/xml,application/xml,application/xhtml+xml...   \n",
       "3  text/xml,application/xml,application/xhtml+xml...   \n",
       "4  text/xml,application/xml,application/xhtml+xml...   \n",
       "\n",
       "                    Accept-Encoding               Accept-Charset  \\\n",
       "0  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "1  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "2  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "3  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "4  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "\n",
       "  Accept-Language    Pragma Connection Body Length  URL Length  \\\n",
       "0              en  no-cache      close           0         195   \n",
       "1              en  no-cache      close           0          48   \n",
       "2              en  no-cache      close           0         126   \n",
       "3              en  no-cache      close           0          48   \n",
       "4              en  no-cache      close           0          40   \n",
       "\n",
       "   Special Characters Count in URL  Special Characters Count in Query Params  \n",
       "0                                3                                        18  \n",
       "1                                3                                         0  \n",
       "2                                3                                        13  \n",
       "3                                3                                         0  \n",
       "4                                2                                         0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_file_parsed = 'anomalous_parsed_data.csv'\n",
    "anomaly_file_parsed=pd.read_csv('anomalous_parsed_data.csv')\n",
    "anomaly_file_parsed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=anomaly_file_parsed.shape[1]\n",
    "n_samples =anomaly_file_parsed.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features: 21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Method                                          0\n",
       "Full URL                                        0\n",
       "URL Path                                      214\n",
       "Query Params                                15468\n",
       "Query Params Length                             0\n",
       "Number of Query Params                          0\n",
       "Body Params                                 25065\n",
       "Content-Length                              15088\n",
       "Content-Type                                15088\n",
       "User-Agent                                      0\n",
       "Host                                            0\n",
       "Accept                                          0\n",
       "Accept-Encoding                                 0\n",
       "Accept-Charset                                  0\n",
       "Accept-Language                                 0\n",
       "Pragma                                          0\n",
       "Connection                                      0\n",
       "Body Length                                     0\n",
       "URL Length                                      0\n",
       "Special Characters Count in URL                 0\n",
       "Special Characters Count in Query Params        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'number of features: {n_features}')\n",
    "missing_values_count = anomaly_file_parsed.isnull().sum()\n",
    "missing_values_count[0:n_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values for Method: 3\n",
      "Number of unique values for Full URL: 8666\n",
      "Number of unique values for URL Path: 1612\n",
      "Number of unique values for Query Params: 7042\n",
      "Number of unique values for Query Params Length: 383\n",
      "Number of unique values for Number of Query Params: 5\n",
      "Number of unique values for Body Params: 0\n",
      "Number of unique values for Content-Length: 382\n",
      "Number of unique values for Content-Type: 1\n",
      "Number of unique values for User-Agent: 1\n",
      "Number of unique values for Host: 2\n",
      "Number of unique values for Accept: 1\n",
      "Number of unique values for Accept-Encoding: 1\n",
      "Number of unique values for Accept-Charset: 1\n",
      "Number of unique values for Accept-Language: 1\n",
      "Number of unique values for Pragma: 1\n",
      "Number of unique values for Connection: 1\n",
      "Number of unique values for Body Length: 1\n",
      "Number of unique values for URL Length: 416\n",
      "Number of unique values for Special Characters Count in URL: 7\n",
      "Number of unique values for Special Characters Count in Query Params: 64\n"
     ]
    }
   ],
   "source": [
    "for feature in anomaly_file_parsed.columns:\n",
    "    if feature in anomaly_file_parsed.columns:\n",
    "        unique_count = anomaly_file_parsed[feature].nunique()\n",
    "        print(f\"Number of unique values for {feature}: {unique_count}\")\n",
    "    else:\n",
    "        print(f\"Column '{feature}' does not exist in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Normal and Anomalous Request Datasets Randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def combine_csv_randomly(file1, file2, output_file):\n",
    "    # Load the CSV files into DataFrames\n",
    "    df1 = pd.read_csv(file1)\n",
    "    df2 = pd.read_csv(file2)\n",
    "\n",
    "    # Shuffle the rows of both DataFrames\n",
    "    df1_shuffled = df1.sample(frac=1).reset_index(drop=True)\n",
    "    df2_shuffled = df2.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Add a new column to identify the source of the data\n",
    "    df1_shuffled['request_type'] = '1'  # normal dataset as 1\n",
    "    df2_shuffled['request_type'] = '0'  # anomalous dataset as 0\n",
    "\n",
    "    # Combine the two DataFrames\n",
    "    combined_df = pd.concat([df1_shuffled, df2_shuffled], ignore_index=True)\n",
    "\n",
    "    # Shuffle the combined DataFrame to mix rows from both sources\n",
    "    combined_df_shuffled = combined_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Save the combined DataFrame to a new CSV file\n",
    "    combined_df_shuffled.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Combined CSV file '{output_file}' has been created.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV file 'combined_parsed_requests.csv' has been created.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "combine_csv_randomly('normal_parsed_data.csv', 'anomalous_parsed_data.csv', 'combined_parsed_requests.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Request Parser for nginx Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_nginx_log(input_file_path, output_file_path):\n",
    "    log_pattern = re.compile(\n",
    "        r'(?P<ip>\\S+) - - \\[(?P<timestamp>.*?)\\] \"(?P<method>\\S+) (?P<path>\\S+) HTTP/\\S+\" (?P<status>\\d{3}) (?P<size>\\d+) \"(?P<referrer>.*?)\" \"(?P<user_agent>.*?)\"'\n",
    "    )\n",
    "\n",
    "    fieldnames = [\n",
    "        'IP Address', 'Timestamp', 'HTTP Method', 'Request Path', 'Status Code', \n",
    "        'Response Size', 'Referrer', 'User Agent', 'Request Length', 'Query Parameters Count', \n",
    "        'Is Secure', 'Time of Day', 'Day of Week', 'User-Agent Length', 'Referrer Length', \n",
    "        'Status Code Category', 'Request Frequency', 'Status Code Distribution', \n",
    "        'Request Size Distribution', 'User-Agent Diversity', 'Time Interval Between Requests', \n",
    "        'Path Frequency', 'Suspicious Patterns', 'SQL Injection Detected', 'XSS Detected',\n",
    "        'Command Injection Detected', 'Insecure Deserialization Detected', 'File Inclusion Detected'\n",
    "    ]\n",
    "\n",
    "    ip_request_times = defaultdict(list)\n",
    "    ip_status_code_counts = defaultdict(lambda: defaultdict(int))\n",
    "    ip_request_sizes = defaultdict(list)\n",
    "    ip_user_agents = defaultdict(set)\n",
    "    ip_path_counts = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "    # OWASP Top 10 attack patterns\n",
    "    sql_injection_pattern = re.compile(r\"('|\\\"|;|--|/\\*|\\*/|SELECT|INSERT|DELETE|UNION|DROP|UPDATE|ALTER)\", re.IGNORECASE)\n",
    "    xss_pattern = re.compile(r\"(<script>|<img|<iframe|onerror|onload|javascript:|<svg)\", re.IGNORECASE)\n",
    "    command_injection_pattern = re.compile(r\"(\\||;|&&|wget|curl|\\$\\(.*\\)|`.*`|bash|sh)\", re.IGNORECASE)\n",
    "    insecure_deserialization_pattern = re.compile(r\"(base64_decode|O:\\d+|s:\\d+|C:\\d+|object|Serialized)\", re.IGNORECASE)\n",
    "    file_inclusion_pattern = re.compile(r\"(\\.\\./|\\.\\./etc/passwd|\\.\\./\\.env|\\.\\./config|\\.\\.php|\\.\\.html)\", re.IGNORECASE)\n",
    "\n",
    "    with open(output_file_path, mode='w', newline='') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        with open(input_file_path, 'r') as log_file:\n",
    "            for log_entry in log_file:\n",
    "                match = log_pattern.match(log_entry)\n",
    "                if match:\n",
    "                    log_data = match.groupdict()\n",
    "\n",
    "                    timestamp = datetime.strptime(log_data['timestamp'], '%d/%b/%Y:%H:%M:%S %z')\n",
    "                    log_data['Timestamp'] = timestamp.isoformat()\n",
    "                    log_data['IP Address'] = log_data.pop('ip')\n",
    "                    log_data['HTTP Method'] = log_data.pop('method')\n",
    "                    log_data['Request Path'] = log_data.pop('path')\n",
    "                    log_data['Status Code'] = log_data.pop('status')\n",
    "                    log_data['Response Size'] = log_data.pop('size')\n",
    "                    log_data['Referrer'] = log_data.pop('referrer')\n",
    "                    log_data['User Agent'] = log_data.pop('user_agent')\n",
    "\n",
    "                    request_length = len(log_entry)\n",
    "                    query_parameters_count = len(re.findall(r'[?&]', log_data['Request Path']))\n",
    "                    is_secure = 'Yes' if log_data['Request Path'].startswith('https://') else 'No'\n",
    "                    time_of_day = timestamp.strftime('%H:%M:%S')\n",
    "                    day_of_week = timestamp.strftime('%A')\n",
    "                    user_agent_length = len(log_data['User Agent'])\n",
    "                    referrer_length = len(log_data['Referrer'])\n",
    "                    status_code_category = f\"{log_data['Status Code'][0]}xx\"\n",
    "                    \n",
    "                    ip_request_times[log_data['IP Address']].append(timestamp)\n",
    "                    ip_status_code_counts[log_data['IP Address']][log_data['Status Code']] += 1\n",
    "                    ip_request_sizes[log_data['IP Address']].append(int(log_data['Response Size']))\n",
    "                    ip_user_agents[log_data['IP Address']].add(log_data['User Agent'])\n",
    "                    ip_path_counts[log_data['IP Address']][log_data['Request Path']] += 1\n",
    "\n",
    "                    request_frequency = len(ip_request_times[log_data['IP Address']])\n",
    "                    status_code_distribution = ip_status_code_counts[log_data['IP Address']]\n",
    "                    request_size_distribution = sum(ip_request_sizes[log_data['IP Address']]) / len(ip_request_sizes[log_data['IP Address']]) if ip_request_sizes[log_data['IP Address']] else 0\n",
    "                    user_agent_diversity = len(ip_user_agents[log_data['IP Address']])\n",
    "                    if len(ip_request_times[log_data['IP Address']]) > 1:\n",
    "                        time_intervals = [ip_request_times[log_data['IP Address']][i] - ip_request_times[log_data['IP Address']][i-1] for i in range(1, len(ip_request_times[log_data['IP Address']]))]\n",
    "                        time_interval_between_requests = sum(time_intervals, timedelta()) / len(time_intervals)\n",
    "                    else:\n",
    "                        time_interval_between_requests = timedelta()\n",
    "                    path_frequency = ip_path_counts[log_data['IP Address']][log_data['Request Path']]\n",
    "\n",
    "                    # Suspicious pattern detection\n",
    "                    suspicious_patterns = ['admin', 'wp-login', 'phpmyadmin']\n",
    "                    suspicious_patterns_found = any(pattern in log_data['Request Path'] for pattern in suspicious_patterns)\n",
    "\n",
    "                    # OWASP Top 10 Detection\n",
    "                    sql_injection_detected = bool(sql_injection_pattern.search(log_data['Request Path']))\n",
    "                    xss_detected = bool(xss_pattern.search(log_data['Request Path'])) or bool(xss_pattern.search(log_data['Referrer']))\n",
    "                    command_injection_detected = bool(command_injection_pattern.search(log_data['Request Path']))\n",
    "                    insecure_deserialization_detected = bool(insecure_deserialization_pattern.search(log_data['Request Path']))\n",
    "                    file_inclusion_detected = bool(file_inclusion_pattern.search(log_data['Request Path']))\n",
    "\n",
    "                    log_data['Request Length'] = request_length\n",
    "                    log_data['Query Parameters Count'] = query_parameters_count\n",
    "                    log_data['Is Secure'] = is_secure\n",
    "                    log_data['Time of Day'] = time_of_day\n",
    "                    log_data['Day of Week'] = day_of_week\n",
    "                    log_data['User-Agent Length'] = user_agent_length\n",
    "                    log_data['Referrer Length'] = referrer_length\n",
    "                    log_data['Status Code Category'] = status_code_category\n",
    "                    log_data['Request Frequency'] = request_frequency\n",
    "                    log_data['Status Code Distribution'] = str(status_code_distribution)\n",
    "                    log_data['Request Size Distribution'] = request_size_distribution\n",
    "                    log_data['User-Agent Diversity'] = user_agent_diversity\n",
    "                    log_data['Time Interval Between Requests'] = str(time_interval_between_requests)\n",
    "                    log_data['Path Frequency'] = path_frequency\n",
    "                    log_data['Suspicious Patterns'] = 'Yes' if suspicious_patterns_found else 'No'\n",
    "                    log_data['SQL Injection Detected'] = 'Yes' if sql_injection_detected else 'No'\n",
    "                    log_data['XSS Detected'] = 'Yes' if xss_detected else 'No'\n",
    "                    log_data['Command Injection Detected'] = 'Yes' if command_injection_detected else 'No'\n",
    "                    log_data['Insecure Deserialization Detected'] = 'Yes' if insecure_deserialization_detected else 'No'\n",
    "                    log_data['File Inclusion Detected'] = 'Yes' if file_inclusion_detected else 'No'\n",
    "\n",
    "                    row_data = {field: log_data.get(field, '') for field in fieldnames}\n",
    "                    writer.writerow(row_data)\n",
    "\n",
    "    print(f\"CSV file '{output_file_path}' has been created successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'access_logs.csv' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "parse_nginx_log('dataset/nginx_logs/nginx_logs/access.log', 'access_logs.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
