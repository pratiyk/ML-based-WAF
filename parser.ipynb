{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "def parse_file(file_in, file_out):\n",
    "    with open(file_in, 'r', encoding='utf-8') as fin, open(file_out, 'w', newline='', encoding='utf-8') as fout:\n",
    "        csv_writer = csv.writer(fout)\n",
    "        # Write the header\n",
    "        csv_writer.writerow([\n",
    "            'Method', 'Full URL', 'URL Path', 'Query Params', 'Content-Length', 'Content-Type',\n",
    "            'User-Agent', 'Host', 'Cookie', 'Accept', 'Accept-Encoding', 'Accept-Charset',\n",
    "            'Accept-Language', 'Pragma', 'Cache-Control', 'Connection', 'Body', 'Timestamp',\n",
    "            'IP Address', 'Status Code'\n",
    "        ])\n",
    "        \n",
    "        lines = fin.readlines()\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i].strip()\n",
    "            if line.startswith(('GET', 'POST', 'PUT', 'DELETE')):\n",
    "                # Extract Method\n",
    "                method = line.split(' ')[0]\n",
    "                # Extract Full URL\n",
    "                full_url = line.split(' ')[1]\n",
    "                # Extract URL Path\n",
    "                url_path = urlparse(full_url).path\n",
    "                # Extract Query Parameters (if any)\n",
    "                query_params = parse_qs(urlparse(full_url).query)\n",
    "                query_params_str = '&'.join([f'{k}={v[0]}' for k, v in query_params.items()])\n",
    "                \n",
    "                # Initialize other fields\n",
    "                content_length = ''\n",
    "                content_type = ''\n",
    "                user_agent = ''\n",
    "                host = ''\n",
    "                cookie = ''\n",
    "                accept = ''\n",
    "                accept_encoding = ''\n",
    "                accept_charset = ''\n",
    "                accept_language = ''\n",
    "                pragma = ''\n",
    "                cache_control = ''\n",
    "                connection = ''\n",
    "                body = ''\n",
    "                timestamp = ''\n",
    "                ip_address = ''  # Assuming IP address is not present in the line\n",
    "                status_code = ''  # Assuming status code is not present in the line\n",
    "                \n",
    "                # Loop through headers and body\n",
    "                j = 1\n",
    "                while i + j < len(lines) and not lines[i + j].strip() == '':\n",
    "                    header_line = lines[i + j].strip()\n",
    "                    if header_line.startswith('Content-Length:'):\n",
    "                        content_length = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Content-Type:'):\n",
    "                        content_type = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('User-Agent:'):\n",
    "                        user_agent = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Host:'):\n",
    "                        host = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Cookie:'):\n",
    "                        cookie = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept:'):\n",
    "                        accept = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Encoding:'):\n",
    "                        accept_encoding = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Charset:'):\n",
    "                        accept_charset = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Language:'):\n",
    "                        accept_language = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Pragma:'):\n",
    "                        pragma = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Cache-Control:'):\n",
    "                        cache_control = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Connection:'):\n",
    "                        connection = header_line.split(':', 1)[1].strip()\n",
    "                    j += 1\n",
    "                \n",
    "                # The body is the line after the headers\n",
    "                if i + j < len(lines):\n",
    "                    body = lines[i + j].strip()\n",
    "                \n",
    "                # Append the extracted features\n",
    "                csv_writer.writerow([\n",
    "                    method, full_url, url_path, query_params_str, content_length, content_type,\n",
    "                    user_agent, host, cookie, accept, accept_encoding, accept_charset,\n",
    "                    accept_language, pragma, cache_control, connection, body, timestamp,\n",
    "                    ip_address, status_code\n",
    "                ])\n",
    "                \n",
    "                i += j + 1\n",
    "            else:\n",
    "                i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parse_file('anomalousTrafficTest.txt', 'anomalous_parsed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file('normalTrafficTraining.txt', 'normal_parsed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "def parse_file(file_in, file_out):\n",
    "    with open(file_in, 'r', encoding='utf-8') as fin, open(file_out, 'w', newline='', encoding='utf-8') as fout:\n",
    "        csv_writer = csv.writer(fout)\n",
    "        # Write the header\n",
    "        csv_writer.writerow([\n",
    "            'Method', 'Full URL', 'URL Path', 'Query Params', 'Body Params', 'Content-Length', \n",
    "            'Content-Type', 'User-Agent', 'Host', 'Cookie', 'Accept', 'Accept-Encoding', \n",
    "            'Accept-Charset', 'Accept-Language', 'Pragma', 'Cache-Control', 'Connection'\n",
    "        ])\n",
    "        \n",
    "        lines = fin.readlines()\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i].strip()\n",
    "            if line.startswith(('GET', 'POST', 'PUT', 'DELETE')):\n",
    "                # Extract Method\n",
    "                method = line.split(' ')[0]\n",
    "                # Extract Full URL\n",
    "                full_url = line.split(' ')[1]\n",
    "                # Extract URL Path\n",
    "                url_path = urlparse(full_url).path\n",
    "                # Extract Query Parameters (if any)\n",
    "                query_params = parse_qs(urlparse(full_url).query)\n",
    "                query_params_str = '; '.join([f'{k}={v[0]}' for k, v in query_params.items()])\n",
    "                \n",
    "                # Initialize other fields\n",
    "                content_length = ''\n",
    "                content_type = ''\n",
    "                user_agent = ''\n",
    "                host = ''\n",
    "                cookie = ''\n",
    "                accept = ''\n",
    "                accept_encoding = ''\n",
    "                accept_charset = ''\n",
    "                accept_language = ''\n",
    "                pragma = ''\n",
    "                cache_control = ''\n",
    "                connection = ''\n",
    "                body_params = ''\n",
    "                \n",
    "                headers = {}\n",
    "                body = ''\n",
    "                \n",
    "                # Loop through headers and body\n",
    "                j = 1\n",
    "                while i + j < len(lines) and not lines[i + j].strip() == '':\n",
    "                    header_line = lines[i + j].strip()\n",
    "                    if header_line.startswith('Content-Length:'):\n",
    "                        content_length = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Content-Type:'):\n",
    "                        content_type = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('User-Agent:'):\n",
    "                        user_agent = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Host:'):\n",
    "                        host = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Cookie:'):\n",
    "                        cookie = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept:'):\n",
    "                        accept = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Encoding:'):\n",
    "                        accept_encoding = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Charset:'):\n",
    "                        accept_charset = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Language:'):\n",
    "                        accept_language = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Pragma:'):\n",
    "                        pragma = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Cache-Control:'):\n",
    "                        cache_control = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Connection:'):\n",
    "                        connection = header_line.split(':', 1)[1].strip()\n",
    "                    else:\n",
    "                        # Collect all header fields\n",
    "                        header_key, header_value = map(str.strip, header_line.split(':', 1))\n",
    "                        headers[header_key] = header_value\n",
    "                    \n",
    "                    j += 1\n",
    "                \n",
    "                # The body is the line after the headers\n",
    "                if i + j < len(lines):\n",
    "                    body = lines[i + j].strip()\n",
    "                    # Extract body parameters (if the body is URL-encoded)\n",
    "                    body_params = parse_qs(body)\n",
    "                    body_params_str = '; '.join([f'{k}={v[0]}' for k, v in body_params.items()])\n",
    "\n",
    "                # Append the extracted features\n",
    "                csv_writer.writerow([\n",
    "                    method, full_url, url_path, query_params_str, body_params_str, content_length, \n",
    "                    content_type, user_agent, host, cookie, accept, accept_encoding, \n",
    "                    accept_charset, accept_language, pragma, cache_control, connection\n",
    "                ])\n",
    "                \n",
    "                i += j + 1\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file('anomalousTrafficTest.txt', 'anomalous_parsed_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file('normalTrafficTraining.txt', 'normal_parsed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removed features such as value of cookie as it is specific to the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_parameters(param_str):\n",
    "    \"\"\"\n",
    "    Extracts parameters from a URL-encoded string and returns as a formatted string.\n",
    "    \"\"\"\n",
    "    params = parse_qs(param_str)\n",
    "    return '; '.join([f'{k}={v[0]}' for k, v in params.items()])\n",
    "\n",
    "def parse_file(file_in, file_out):\n",
    "    with open(file_in, 'r', encoding='utf-8') as fin, open(file_out, 'w', newline='', encoding='utf-8') as fout:\n",
    "        csv_writer = csv.writer(fout)\n",
    "        # Write the header\n",
    "        csv_writer.writerow([\n",
    "            'Method', 'Full URL', 'URL Path', 'Query Params', 'Body Params', \n",
    "            'Content-Length', 'Content-Type', 'User-Agent', 'Host','Accept', \n",
    "            'Accept-Encoding', 'Accept-Charset', 'Accept-Language', 'Pragma', 'Cache-Control', \n",
    "            'Connection', 'Body Length', 'URL Length'\n",
    "        ])\n",
    "        \n",
    "        lines = fin.readlines()\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i].strip()\n",
    "            if line.startswith(('GET', 'POST', 'PUT', 'DELETE')):\n",
    "                # Example timestamp extraction (assuming it's available in the log)\n",
    "                # This part needs to be adapted to your log format if a timestamp is present.\n",
    "                timestamp = 'N/A'  # Placeholder for timestamp extraction\n",
    "                \n",
    "                # Extract Method\n",
    "                method = line.split(' ')[0]\n",
    "                \n",
    "                # Extract Full URL\n",
    "                full_url = line.split(' ')[1]\n",
    "                \n",
    "                # Extract URL Path\n",
    "                url_path = urlparse(full_url).path\n",
    "                \n",
    "                # Extract Query Parameters\n",
    "                query_params_str = extract_parameters(urlparse(full_url).query)\n",
    "                \n",
    "                # Initialize other fields\n",
    "                content_length = ''\n",
    "                content_type = ''\n",
    "                user_agent = ''\n",
    "                host = ''\n",
    "                accept = ''\n",
    "                accept_encoding = ''\n",
    "                accept_charset = ''\n",
    "                accept_language = ''\n",
    "                pragma = ''\n",
    "                cache_control = ''\n",
    "                connection = ''\n",
    "                body_params_str = ''\n",
    "                \n",
    "                headers = {}\n",
    "                body = ''\n",
    "                \n",
    "                # Loop through headers and body\n",
    "                j = 1\n",
    "                while i + j < len(lines) and not lines[i + j].strip() == '':\n",
    "                    header_line = lines[i + j].strip()\n",
    "                    if header_line.startswith('Content-Length:'):\n",
    "                        content_length = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Content-Type:'):\n",
    "                        content_type = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('User-Agent:'):\n",
    "                        user_agent = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Host:'):\n",
    "                        host = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept:'):\n",
    "                        accept = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Encoding:'):\n",
    "                        accept_encoding = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Charset:'):\n",
    "                        accept_charset = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Language:'):\n",
    "                        accept_language = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Pragma:'):\n",
    "                        pragma = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Cache-Control:'):\n",
    "                        cache_control = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Connection:'):\n",
    "                        connection = header_line.split(':', 1)[1].strip()\n",
    "                    j += 1\n",
    "                \n",
    "                # The body is the line after the headers\n",
    "                if i + j < len(lines):\n",
    "                    body = lines[i + j].strip()\n",
    "                    # Extract body parameters (if the body is URL-encoded)\n",
    "                    body_params_str = extract_parameters(body)\n",
    "                \n",
    "                # Calculate lengths for additional features\n",
    "                body_length = len(body)\n",
    "                url_length = len(full_url)\n",
    "                    \n",
    "                # Append the extracted features to the CSV\n",
    "                csv_writer.writerow([\n",
    "                    method, full_url, url_path, query_params_str, body_params_str, \n",
    "                    content_length, content_type, user_agent, host, accept, \n",
    "                    accept_encoding, accept_charset, accept_language, pragma, cache_control, \n",
    "                    connection, body_length, url_length\n",
    "                ])\n",
    "                \n",
    "                i += j + 1\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file('anomalousTrafficTest.txt', 'anomalous_parsed_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file('normalTrafficTraining.txt', 'normal_parsed_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
