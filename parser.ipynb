{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "def parse_file(file_in, file_out):\n",
    "    with open(file_in, 'r', encoding='utf-8') as fin, open(file_out, 'w', newline='', encoding='utf-8') as fout:\n",
    "        csv_writer = csv.writer(fout)\n",
    "        # Write the header\n",
    "        csv_writer.writerow([\n",
    "            'Method', 'Full URL', 'URL Path', 'Query Params', 'Content-Length', 'Content-Type',\n",
    "            'User-Agent', 'Host', 'Cookie', 'Accept', 'Accept-Encoding', 'Accept-Charset',\n",
    "            'Accept-Language', 'Pragma', 'Cache-Control', 'Connection', 'Body', 'Timestamp',\n",
    "            'IP Address', 'Status Code'\n",
    "        ])\n",
    "        \n",
    "        lines = fin.readlines()\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i].strip()\n",
    "            if line.startswith(('GET', 'POST', 'PUT', 'DELETE')):\n",
    "                # Extract Method\n",
    "                method = line.split(' ')[0]\n",
    "                # Extract Full URL\n",
    "                full_url = line.split(' ')[1]\n",
    "                # Extract URL Path\n",
    "                url_path = urlparse(full_url).path\n",
    "                # Extract Query Parameters (if any)\n",
    "                query_params = parse_qs(urlparse(full_url).query)\n",
    "                query_params_str = '&'.join([f'{k}={v[0]}' for k, v in query_params.items()])\n",
    "                \n",
    "                # Initialize other fields\n",
    "                content_length = ''\n",
    "                content_type = ''\n",
    "                user_agent = ''\n",
    "                host = ''\n",
    "                cookie = ''\n",
    "                accept = ''\n",
    "                accept_encoding = ''\n",
    "                accept_charset = ''\n",
    "                accept_language = ''\n",
    "                pragma = ''\n",
    "                cache_control = ''\n",
    "                connection = ''\n",
    "                body = ''\n",
    "                timestamp = ''\n",
    "                ip_address = ''  # Assuming IP address is not present in the line\n",
    "                status_code = ''  # Assuming status code is not present in the line\n",
    "                \n",
    "                # Loop through headers and body\n",
    "                j = 1\n",
    "                while i + j < len(lines) and not lines[i + j].strip() == '':\n",
    "                    header_line = lines[i + j].strip()\n",
    "                    if header_line.startswith('Content-Length:'):\n",
    "                        content_length = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Content-Type:'):\n",
    "                        content_type = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('User-Agent:'):\n",
    "                        user_agent = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Host:'):\n",
    "                        host = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Cookie:'):\n",
    "                        cookie = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept:'):\n",
    "                        accept = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Encoding:'):\n",
    "                        accept_encoding = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Charset:'):\n",
    "                        accept_charset = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Language:'):\n",
    "                        accept_language = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Pragma:'):\n",
    "                        pragma = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Cache-Control:'):\n",
    "                        cache_control = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Connection:'):\n",
    "                        connection = header_line.split(':', 1)[1].strip()\n",
    "                    j += 1\n",
    "                \n",
    "                # The body is the line after the headers\n",
    "                if i + j < len(lines):\n",
    "                    body = lines[i + j].strip()\n",
    "                \n",
    "                # Append the extracted features\n",
    "                csv_writer.writerow([\n",
    "                    method, full_url, url_path, query_params_str, content_length, content_type,\n",
    "                    user_agent, host, cookie, accept, accept_encoding, accept_charset,\n",
    "                    accept_language, pragma, cache_control, connection, body, timestamp,\n",
    "                    ip_address, status_code\n",
    "                ])\n",
    "                \n",
    "                i += j + 1\n",
    "            else:\n",
    "                i += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parse_file('anomalousTrafficTest.txt', 'anomalous_parsed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file('normalTrafficTraining.txt', 'normal_parsed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "\n",
    "def parse_file(file_in, file_out):\n",
    "    with open(file_in, 'r', encoding='utf-8') as fin, open(file_out, 'w', newline='', encoding='utf-8') as fout:\n",
    "        csv_writer = csv.writer(fout)\n",
    "        # Write the header\n",
    "        csv_writer.writerow([\n",
    "            'Method', 'Full URL', 'URL Path', 'Query Params', 'Body Params', 'Content-Length', \n",
    "            'Content-Type', 'User-Agent', 'Host', 'Cookie', 'Accept', 'Accept-Encoding', \n",
    "            'Accept-Charset', 'Accept-Language', 'Pragma', 'Cache-Control', 'Connection'\n",
    "        ])\n",
    "        \n",
    "        lines = fin.readlines()\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i].strip()\n",
    "            if line.startswith(('GET', 'POST', 'PUT', 'DELETE')):\n",
    "                # Extract Method\n",
    "                method = line.split(' ')[0]\n",
    "                # Extract Full URL\n",
    "                full_url = line.split(' ')[1]\n",
    "                # Extract URL Path\n",
    "                url_path = urlparse(full_url).path\n",
    "                # Extract Query Parameters (if any)\n",
    "                query_params = parse_qs(urlparse(full_url).query)\n",
    "                query_params_str = '; '.join([f'{k}={v[0]}' for k, v in query_params.items()])\n",
    "                \n",
    "                # Initialize other fields\n",
    "                content_length = ''\n",
    "                content_type = ''\n",
    "                user_agent = ''\n",
    "                host = ''\n",
    "                cookie = ''\n",
    "                accept = ''\n",
    "                accept_encoding = ''\n",
    "                accept_charset = ''\n",
    "                accept_language = ''\n",
    "                pragma = ''\n",
    "                cache_control = ''\n",
    "                connection = ''\n",
    "                body_params = ''\n",
    "                \n",
    "                headers = {}\n",
    "                body = ''\n",
    "                \n",
    "                # Loop through headers and body\n",
    "                j = 1\n",
    "                while i + j < len(lines) and not lines[i + j].strip() == '':\n",
    "                    header_line = lines[i + j].strip()\n",
    "                    if header_line.startswith('Content-Length:'):\n",
    "                        content_length = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Content-Type:'):\n",
    "                        content_type = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('User-Agent:'):\n",
    "                        user_agent = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Host:'):\n",
    "                        host = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Cookie:'):\n",
    "                        cookie = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept:'):\n",
    "                        accept = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Encoding:'):\n",
    "                        accept_encoding = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Charset:'):\n",
    "                        accept_charset = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Language:'):\n",
    "                        accept_language = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Pragma:'):\n",
    "                        pragma = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Cache-Control:'):\n",
    "                        cache_control = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Connection:'):\n",
    "                        connection = header_line.split(':', 1)[1].strip()\n",
    "                    else:\n",
    "                        # Collect all header fields\n",
    "                        header_key, header_value = map(str.strip, header_line.split(':', 1))\n",
    "                        headers[header_key] = header_value\n",
    "                    \n",
    "                    j += 1\n",
    "                \n",
    "                # The body is the line after the headers\n",
    "                if i + j < len(lines):\n",
    "                    body = lines[i + j].strip()\n",
    "                    # Extract body parameters (if the body is URL-encoded)\n",
    "                    body_params = parse_qs(body)\n",
    "                    body_params_str = '; '.join([f'{k}={v[0]}' for k, v in body_params.items()])\n",
    "\n",
    "                # Append the extracted features\n",
    "                csv_writer.writerow([\n",
    "                    method, full_url, url_path, query_params_str, body_params_str, content_length, \n",
    "                    content_type, user_agent, host, cookie, accept, accept_encoding, \n",
    "                    accept_charset, accept_language, pragma, cache_control, connection\n",
    "                ])\n",
    "                \n",
    "                i += j + 1\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file('anomalousTrafficTest.txt', 'anomalous_parsed_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file('normalTrafficTraining.txt', 'normal_parsed_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "removed features such as value of cookie as it is specific to the request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_parameters(param_str):\n",
    "    \"\"\"\n",
    "    Extracts parameters from a URL-encoded string and returns as a formatted string.\n",
    "    \"\"\"\n",
    "    params = parse_qs(param_str)\n",
    "    return '; '.join([f'{k}={v[0]}' for k, v in params.items()])\n",
    "\n",
    "def parse_file(file_in, file_out):\n",
    "    with open(file_in, 'r', encoding='utf-8') as fin, open(file_out, 'w', newline='', encoding='utf-8') as fout:\n",
    "        csv_writer = csv.writer(fout)\n",
    "        # Write the header\n",
    "        csv_writer.writerow([\n",
    "            'Method', 'Full URL', 'URL Path', 'Query Params', 'Body Params', \n",
    "            'Content-Length', 'Content-Type', 'User-Agent', 'Host','Accept', \n",
    "            'Accept-Encoding', 'Accept-Charset', 'Accept-Language', 'Pragma', 'Cache-Control', \n",
    "            'Connection', 'Body Length', 'URL Length'\n",
    "        ])\n",
    "        \n",
    "        lines = fin.readlines()\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i].strip()\n",
    "            if line.startswith(('GET', 'POST', 'PUT', 'DELETE', 'HEAD', 'TRACE', 'PATCH', 'OPTIONS')):\n",
    "                # Example timestamp extraction (assuming it's available in the log)\n",
    "                # This part needs to be adapted to your log format if a timestamp is present.\n",
    "                timestamp = 'N/A'  # Placeholder for timestamp extraction\n",
    "                \n",
    "                # Extract Method\n",
    "                method = line.split(' ')[0]\n",
    "                \n",
    "                # Extract Full URL\n",
    "                full_url = line.split(' ')[1]\n",
    "                \n",
    "                # Extract URL Path\n",
    "                url_path = urlparse(full_url).path\n",
    "                \n",
    "                # Extract Query Parameters\n",
    "                query_params_str = extract_parameters(urlparse(full_url).query)\n",
    "                \n",
    "                # Initialize other fields\n",
    "                content_length = ''\n",
    "                content_type = ''\n",
    "                user_agent = ''\n",
    "                host = ''\n",
    "                accept = ''\n",
    "                accept_encoding = ''\n",
    "                accept_charset = ''\n",
    "                accept_language = ''\n",
    "                pragma = ''\n",
    "                cache_control = ''\n",
    "                connection = ''\n",
    "                body_params_str = ''\n",
    "                \n",
    "                headers = {}\n",
    "                body = ''\n",
    "                \n",
    "                # Loop through headers and body\n",
    "                j = 1\n",
    "                while i + j < len(lines) and not lines[i + j].strip() == '':\n",
    "                    header_line = lines[i + j].strip()\n",
    "                    if header_line.startswith('Content-Length:'):\n",
    "                        content_length = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Content-Type:'):\n",
    "                        content_type = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('User-Agent:'):\n",
    "                        user_agent = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Host:'):\n",
    "                        host = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept:'):\n",
    "                        accept = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Encoding:'):\n",
    "                        accept_encoding = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Charset:'):\n",
    "                        accept_charset = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Language:'):\n",
    "                        accept_language = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Pragma:'):\n",
    "                        pragma = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Cache-Control:'):\n",
    "                        cache_control = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Connection:'):\n",
    "                        connection = header_line.split(':', 1)[1].strip()\n",
    "                    j += 1\n",
    "                \n",
    "                # The body is the line after the headers\n",
    "                if i + j < len(lines):\n",
    "                    body = lines[i + j].strip()\n",
    "                    # Extract body parameters (if the body is URL-encoded)\n",
    "                    body_params_str = extract_parameters(body)\n",
    "                \n",
    "                # Calculate lengths for additional features\n",
    "                body_length = len(body)\n",
    "                url_length = len(full_url)\n",
    "                    \n",
    "                # Append the extracted features to the CSV\n",
    "                csv_writer.writerow([\n",
    "                    method, full_url, url_path, query_params_str, body_params_str, \n",
    "                    content_length, content_type, user_agent, host, accept, \n",
    "                    accept_encoding, accept_charset, accept_language, pragma, cache_control, \n",
    "                    connection, body_length, url_length\n",
    "                ])\n",
    "                \n",
    "                i += j + 1\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file('anomalousTrafficTest.txt', 'anomalous_parsed_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file('normalTrafficTraining.txt', 'normal_parsed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import xml.etree.ElementTree as ET\n",
    "from urllib.parse import urlparse, parse_qs\n",
    "from datetime import datetime\n",
    "\n",
    "def extract_parameters(param_str):\n",
    "    \"\"\"\n",
    "    Extracts parameters from a URL-encoded string and returns as a formatted string.\n",
    "    \"\"\"\n",
    "    params = parse_qs(param_str)\n",
    "    return '; '.join([f'{k}={v[0]}' for k, v in params.items()])\n",
    "\n",
    "def has_special_characters(s):\n",
    "    \"\"\"\n",
    "    Checks if the string contains special characters that could be used in SQL injection.\n",
    "    \"\"\"\n",
    "    sql_injection_patterns = [\n",
    "        r\"'\",       # Single quote\n",
    "        r\"--\",      # SQL comment\n",
    "        r\"\\*\",      # Asterisk\n",
    "        r\"#\",       # Hash (another comment style)\n",
    "        r\"\\bSELECT\\b\",   # SQL SELECT statement\n",
    "        r\"\\bUNION\\b\",    # SQL UNION keyword\n",
    "        r\"\\bINSERT\\b\",   # SQL INSERT statement\n",
    "        r\"\\bUPDATE\\b\",   # SQL UPDATE statement\n",
    "        r\"\\bDELETE\\b\",   # SQL DELETE statement\n",
    "        r\"\\bDROP\\b\",     # SQL DROP statement\n",
    "        r\"\\bFROM\\b\",     # SQL FROM keyword\n",
    "        r\"\\bWHERE\\b\"     # SQL WHERE keyword\n",
    "    ]\n",
    "    \n",
    "    return any(re.search(pattern, s, re.IGNORECASE) for pattern in sql_injection_patterns)\n",
    "\n",
    "def parse_txt_file(file_in):\n",
    "    \"\"\"\n",
    "    Parses a .txt file and returns a list of log entries.\n",
    "    \"\"\"\n",
    "    with open(file_in, 'r', encoding='utf-8') as fin:\n",
    "        return fin.readlines()\n",
    "\n",
    "def parse_xml_file(file_in):\n",
    "    \"\"\"\n",
    "    Parses a .xml file and returns a list of log entries.\n",
    "    \"\"\"\n",
    "    tree = ET.parse(file_in)\n",
    "    root = tree.getroot()\n",
    "    logs = []\n",
    "\n",
    "    for log_entry in root.findall('.//log_entry'):\n",
    "        log_lines = []\n",
    "        for line in log_entry.findall('line'):\n",
    "            log_lines.append(line.text.strip())\n",
    "        logs.append('\\n'.join(log_lines))\n",
    "    \n",
    "    return logs\n",
    "\n",
    "def parse_file(file_in, file_out):\n",
    "    # Determine file type\n",
    "    if file_in.endswith('.txt'):\n",
    "        lines = parse_txt_file(file_in)\n",
    "    elif file_in.endswith('.xml'):\n",
    "        lines = parse_xml_file(file_in)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file type. Only .txt and .xml files are supported.\")\n",
    "\n",
    "    with open(file_out, 'w', newline='', encoding='utf-8') as fout:\n",
    "        csv_writer = csv.writer(fout)\n",
    "        # Write the header\n",
    "        csv_writer.writerow([\n",
    "            'Method', 'Full URL', 'URL Path', 'Query Params', 'Query Params Length', 'Number of Query Params', 'Body Params', \n",
    "            'Content-Length', 'Content-Type', 'User-Agent', 'Host', 'Accept', \n",
    "            'Accept-Encoding', 'Accept-Charset', 'Accept-Language', 'Pragma', 'Cache-Control', \n",
    "            'Connection', 'Body Length', 'URL Length', 'Special Characters in URL', 'Special Characters in Query Params'\n",
    "        ])\n",
    "        \n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i].strip()\n",
    "            if line.startswith(('GET', 'POST', 'PUT', 'DELETE', 'HEAD', 'TRACE', 'PATCH', 'OPTIONS')):\n",
    "                # Example timestamp extraction (assuming it's available in the log)\n",
    "                timestamp = 'N/A'  # Placeholder for timestamp extraction\n",
    "                \n",
    "                # Extract Method\n",
    "                method = line.split(' ')[0]\n",
    "                \n",
    "                # Extract Full URL\n",
    "                full_url = line.split(' ')[1]\n",
    "                \n",
    "                # Extract URL Path\n",
    "                url_path = urlparse(full_url).path\n",
    "                \n",
    "                # Extract Query Parameters\n",
    "                query_params = parse_qs(urlparse(full_url).query)\n",
    "                query_params_str = extract_parameters(urlparse(full_url).query)\n",
    "                query_params_length = len(urlparse(full_url).query)\n",
    "                num_query_params = len(query_params)\n",
    "                \n",
    "                # Check for special characters\n",
    "                special_chars_in_url = has_special_characters(url_path)\n",
    "                special_chars_in_query_params = any(has_special_characters(v[0]) for v in query_params.values())\n",
    "                \n",
    "                # Initialize other fields\n",
    "                content_length = ''\n",
    "                content_type = ''\n",
    "                user_agent = ''\n",
    "                host = ''\n",
    "                accept = ''\n",
    "                accept_encoding = ''\n",
    "                accept_charset = ''\n",
    "                accept_language = ''\n",
    "                pragma = ''\n",
    "                cache_control = ''\n",
    "                connection = ''\n",
    "                body_params_str = ''\n",
    "                \n",
    "                headers = {}\n",
    "                body = ''\n",
    "                \n",
    "                # Loop through headers and body\n",
    "                j = 1\n",
    "                while i + j < len(lines) and not lines[i + j].strip() == '':\n",
    "                    header_line = lines[i + j].strip()\n",
    "                    if header_line.startswith('Content-Length:'):\n",
    "                        content_length = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Content-Type:'):\n",
    "                        content_type = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('User-Agent:'):\n",
    "                        user_agent = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Host:'):\n",
    "                        host = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept:'):\n",
    "                        accept = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Encoding:'):\n",
    "                        accept_encoding = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Charset:'):\n",
    "                        accept_charset = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Accept-Language:'):\n",
    "                        accept_language = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Pragma:'):\n",
    "                        pragma = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Cache-Control:'):\n",
    "                        cache_control = header_line.split(':', 1)[1].strip()\n",
    "                    elif header_line.startswith('Connection:'):\n",
    "                        connection = header_line.split(':', 1)[1].strip()\n",
    "                    j += 1\n",
    "                \n",
    "                # The body is the line after the headers\n",
    "                if i + j < len(lines):\n",
    "                    body = lines[i + j].strip()\n",
    "                    # Extract body parameters (if the body is URL-encoded)\n",
    "                    body_params_str = extract_parameters(body)\n",
    "                \n",
    "                # Calculate lengths for additional features\n",
    "                body_length = len(body)\n",
    "                url_length = len(full_url)\n",
    "                    \n",
    "                # Append the extracted features to the CSV\n",
    "                csv_writer.writerow([\n",
    "                    method, full_url, url_path, query_params_str, query_params_length, num_query_params, body_params_str, \n",
    "                    content_length, content_type, user_agent, host, accept, \n",
    "                    accept_encoding, accept_charset, accept_language, pragma, cache_control, \n",
    "                    connection, body_length, url_length, special_chars_in_url, special_chars_in_query_params\n",
    "                ])\n",
    "                \n",
    "                i += j + 1\n",
    "            else:\n",
    "                i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file('anomalousTrafficTest.txt', 'anomalous_parsed_data2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_file_parsed = 'anomalous_parsed_data2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_file_parsed=pd.read_csv('anomalous_parsed_data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Full URL</th>\n",
       "      <th>URL Path</th>\n",
       "      <th>Query Params</th>\n",
       "      <th>Query Params Length</th>\n",
       "      <th>Number of Query Params</th>\n",
       "      <th>Body Params</th>\n",
       "      <th>Content-Length</th>\n",
       "      <th>Content-Type</th>\n",
       "      <th>User-Agent</th>\n",
       "      <th>...</th>\n",
       "      <th>Accept-Encoding</th>\n",
       "      <th>Accept-Charset</th>\n",
       "      <th>Accept-Language</th>\n",
       "      <th>Pragma</th>\n",
       "      <th>Cache-Control</th>\n",
       "      <th>Connection</th>\n",
       "      <th>Body Length</th>\n",
       "      <th>URL Length</th>\n",
       "      <th>Special Characters in URL</th>\n",
       "      <th>Special Characters in Query Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/tienda1/publico/anadir.j...</td>\n",
       "      <td>/tienda1/publico/anadir.jsp</td>\n",
       "      <td>id=2; nombre=Jam�n Ib�rico; precio=85; cantida...</td>\n",
       "      <td>146</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>NaN</td>\n",
       "      <td>close</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>POST</td>\n",
       "      <td>http://localhost:8080/tienda1/publico/anadir.jsp</td>\n",
       "      <td>/tienda1/publico/anadir.jsp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>146.0</td>\n",
       "      <td>application/x-www-form-urlencoded</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>NaN</td>\n",
       "      <td>close</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/tienda1/publico/anadir.j...</td>\n",
       "      <td>/tienda1/publico/anadir.jsp</td>\n",
       "      <td>id=2/; nombre=Jam�n Ib�rico; precio=85; cantid...</td>\n",
       "      <td>77</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>NaN</td>\n",
       "      <td>close</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>POST</td>\n",
       "      <td>http://localhost:8080/tienda1/publico/anadir.jsp</td>\n",
       "      <td>/tienda1/publico/anadir.jsp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>77.0</td>\n",
       "      <td>application/x-www-form-urlencoded</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>NaN</td>\n",
       "      <td>close</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/asf-logo-wide.gif~</td>\n",
       "      <td>/asf-logo-wide.gif~</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>NaN</td>\n",
       "      <td>close</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Method                                           Full URL  \\\n",
       "0    GET  http://localhost:8080/tienda1/publico/anadir.j...   \n",
       "1   POST   http://localhost:8080/tienda1/publico/anadir.jsp   \n",
       "2    GET  http://localhost:8080/tienda1/publico/anadir.j...   \n",
       "3   POST   http://localhost:8080/tienda1/publico/anadir.jsp   \n",
       "4    GET           http://localhost:8080/asf-logo-wide.gif~   \n",
       "\n",
       "                      URL Path  \\\n",
       "0  /tienda1/publico/anadir.jsp   \n",
       "1  /tienda1/publico/anadir.jsp   \n",
       "2  /tienda1/publico/anadir.jsp   \n",
       "3  /tienda1/publico/anadir.jsp   \n",
       "4          /asf-logo-wide.gif~   \n",
       "\n",
       "                                        Query Params  Query Params Length  \\\n",
       "0  id=2; nombre=Jam�n Ib�rico; precio=85; cantida...                  146   \n",
       "1                                                NaN                    0   \n",
       "2  id=2/; nombre=Jam�n Ib�rico; precio=85; cantid...                   77   \n",
       "3                                                NaN                    0   \n",
       "4                                                NaN                    0   \n",
       "\n",
       "   Number of Query Params  Body Params  Content-Length  \\\n",
       "0                       5          NaN             NaN   \n",
       "1                       0          NaN           146.0   \n",
       "2                       5          NaN             NaN   \n",
       "3                       0          NaN            77.0   \n",
       "4                       0          NaN             NaN   \n",
       "\n",
       "                        Content-Type  \\\n",
       "0                                NaN   \n",
       "1  application/x-www-form-urlencoded   \n",
       "2                                NaN   \n",
       "3  application/x-www-form-urlencoded   \n",
       "4                                NaN   \n",
       "\n",
       "                                          User-Agent  ...  \\\n",
       "0  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  ...   \n",
       "1  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  ...   \n",
       "2  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  ...   \n",
       "3  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  ...   \n",
       "4  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  ...   \n",
       "\n",
       "                    Accept-Encoding               Accept-Charset  \\\n",
       "0  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "1  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "2  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "3  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "4  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "\n",
       "  Accept-Language    Pragma Cache-Control Connection  Body Length URL Length  \\\n",
       "0              en  no-cache           NaN      close            0        195   \n",
       "1              en  no-cache           NaN      close            0         48   \n",
       "2              en  no-cache           NaN      close            0        126   \n",
       "3              en  no-cache           NaN      close            0         48   \n",
       "4              en  no-cache           NaN      close            0         40   \n",
       "\n",
       "   Special Characters in URL  Special Characters in Query Params  \n",
       "0                      False                                True  \n",
       "1                      False                               False  \n",
       "2                      False                               False  \n",
       "3                      False                               False  \n",
       "4                      False                               False  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_file_parsed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=anomaly_file_parsed.shape[1]\n",
    "n_samples =anomaly_file_parsed.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features: 22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Method                                    0\n",
       "Full URL                                  0\n",
       "URL Path                                214\n",
       "Query Params                          15468\n",
       "Query Params Length                       0\n",
       "Number of Query Params                    0\n",
       "Body Params                           25065\n",
       "Content-Length                        15088\n",
       "Content-Type                          15088\n",
       "User-Agent                                0\n",
       "Host                                      0\n",
       "Accept                                    0\n",
       "Accept-Encoding                           0\n",
       "Accept-Charset                            0\n",
       "Accept-Language                           0\n",
       "Pragma                                    0\n",
       "Cache-Control                         25065\n",
       "Connection                                0\n",
       "Body Length                               0\n",
       "URL Length                                0\n",
       "Special Characters in URL                 0\n",
       "Special Characters in Query Params        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'number of features: {n_features}')\n",
    "missing_values_count = anomaly_file_parsed.isnull().sum()\n",
    "missing_values_count[0:n_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage missing: 17.41 %\n"
     ]
    }
   ],
   "source": [
    "total_cells = np.prod(anomaly_file_parsed.shape)\n",
    "total_missing = missing_values_count.sum()\n",
    "percent_missing = (total_missing/total_cells) * 100\n",
    "print('percentage missing:',(f'{percent_missing:.2f}') ,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values for Method: 3\n",
      "Number of unique values for Full URL: 8666\n",
      "Number of unique values for URL Path: 1612\n",
      "Number of unique values for Query Params: 7042\n",
      "Number of unique values for Query Params Length: 383\n",
      "Number of unique values for Number of Query Params: 5\n",
      "Number of unique values for Body Params: 0\n",
      "Number of unique values for Content-Length: 382\n",
      "Number of unique values for Content-Type: 1\n",
      "Number of unique values for User-Agent: 1\n",
      "Number of unique values for Host: 2\n",
      "Number of unique values for Accept: 1\n",
      "Number of unique values for Accept-Encoding: 1\n",
      "Number of unique values for Accept-Charset: 1\n",
      "Number of unique values for Accept-Language: 1\n",
      "Number of unique values for Pragma: 1\n",
      "Number of unique values for Cache-Control: 0\n",
      "Number of unique values for Connection: 1\n",
      "Number of unique values for Body Length: 1\n",
      "Number of unique values for URL Length: 416\n",
      "Number of unique values for Special Characters in URL: 2\n",
      "Number of unique values for Special Characters in Query Params: 2\n"
     ]
    }
   ],
   "source": [
    "for feature in anomaly_file_parsed.columns:\n",
    "    if feature in anomaly_file_parsed.columns:\n",
    "        unique_count = anomaly_file_parsed[feature].nunique()\n",
    "        print(f\"Number of unique values for {feature}: {unique_count}\")\n",
    "    else:\n",
    "        print(f\"Column '{feature}' does not exist in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25065"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anomaly_file_parsed.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_file('normalTrafficTraining.txt', 'normal_parsed_data2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_file_parsed = 'normal_parsed_data2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_file_parsed=pd.read_csv('normal_parsed_data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Full URL</th>\n",
       "      <th>URL Path</th>\n",
       "      <th>Query Params</th>\n",
       "      <th>Query Params Length</th>\n",
       "      <th>Number of Query Params</th>\n",
       "      <th>Body Params</th>\n",
       "      <th>Content-Length</th>\n",
       "      <th>Content-Type</th>\n",
       "      <th>User-Agent</th>\n",
       "      <th>...</th>\n",
       "      <th>Accept-Encoding</th>\n",
       "      <th>Accept-Charset</th>\n",
       "      <th>Accept-Language</th>\n",
       "      <th>Pragma</th>\n",
       "      <th>Cache-Control</th>\n",
       "      <th>Connection</th>\n",
       "      <th>Body Length</th>\n",
       "      <th>URL Length</th>\n",
       "      <th>Special Characters in URL</th>\n",
       "      <th>Special Characters in Query Params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/tienda1/index.jsp</td>\n",
       "      <td>/tienda1/index.jsp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>NaN</td>\n",
       "      <td>close</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/tienda1/publico/anadir.j...</td>\n",
       "      <td>/tienda1/publico/anadir.jsp</td>\n",
       "      <td>id=3; nombre=Vino Rioja; precio=100; cantidad=...</td>\n",
       "      <td>68</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>NaN</td>\n",
       "      <td>close</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>POST</td>\n",
       "      <td>http://localhost:8080/tienda1/publico/anadir.jsp</td>\n",
       "      <td>/tienda1/publico/anadir.jsp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>68.0</td>\n",
       "      <td>application/x-www-form-urlencoded</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>NaN</td>\n",
       "      <td>close</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GET</td>\n",
       "      <td>http://localhost:8080/tienda1/publico/autentic...</td>\n",
       "      <td>/tienda1/publico/autenticar.jsp</td>\n",
       "      <td>modo=entrar; login=choong; pwd=d1se3ci�n; reme...</td>\n",
       "      <td>63</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>NaN</td>\n",
       "      <td>close</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>POST</td>\n",
       "      <td>http://localhost:8080/tienda1/publico/autentic...</td>\n",
       "      <td>/tienda1/publico/autenticar.jsp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.0</td>\n",
       "      <td>application/x-www-form-urlencoded</td>\n",
       "      <td>Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...</td>\n",
       "      <td>...</td>\n",
       "      <td>x-gzip, x-deflate, gzip, deflate</td>\n",
       "      <td>utf-8, utf-8;q=0.5, *;q=0.5</td>\n",
       "      <td>en</td>\n",
       "      <td>no-cache</td>\n",
       "      <td>NaN</td>\n",
       "      <td>close</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Method                                           Full URL  \\\n",
       "0    GET            http://localhost:8080/tienda1/index.jsp   \n",
       "1    GET  http://localhost:8080/tienda1/publico/anadir.j...   \n",
       "2   POST   http://localhost:8080/tienda1/publico/anadir.jsp   \n",
       "3    GET  http://localhost:8080/tienda1/publico/autentic...   \n",
       "4   POST  http://localhost:8080/tienda1/publico/autentic...   \n",
       "\n",
       "                          URL Path  \\\n",
       "0               /tienda1/index.jsp   \n",
       "1      /tienda1/publico/anadir.jsp   \n",
       "2      /tienda1/publico/anadir.jsp   \n",
       "3  /tienda1/publico/autenticar.jsp   \n",
       "4  /tienda1/publico/autenticar.jsp   \n",
       "\n",
       "                                        Query Params  Query Params Length  \\\n",
       "0                                                NaN                    0   \n",
       "1  id=3; nombre=Vino Rioja; precio=100; cantidad=...                   68   \n",
       "2                                                NaN                    0   \n",
       "3  modo=entrar; login=choong; pwd=d1se3ci�n; reme...                   63   \n",
       "4                                                NaN                    0   \n",
       "\n",
       "   Number of Query Params  Body Params  Content-Length  \\\n",
       "0                       0          NaN             NaN   \n",
       "1                       5          NaN             NaN   \n",
       "2                       0          NaN            68.0   \n",
       "3                       5          NaN             NaN   \n",
       "4                       0          NaN            63.0   \n",
       "\n",
       "                        Content-Type  \\\n",
       "0                                NaN   \n",
       "1                                NaN   \n",
       "2  application/x-www-form-urlencoded   \n",
       "3                                NaN   \n",
       "4  application/x-www-form-urlencoded   \n",
       "\n",
       "                                          User-Agent  ...  \\\n",
       "0  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  ...   \n",
       "1  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  ...   \n",
       "2  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  ...   \n",
       "3  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  ...   \n",
       "4  Mozilla/5.0 (compatible; Konqueror/3.5; Linux)...  ...   \n",
       "\n",
       "                    Accept-Encoding               Accept-Charset  \\\n",
       "0  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "1  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "2  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "3  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "4  x-gzip, x-deflate, gzip, deflate  utf-8, utf-8;q=0.5, *;q=0.5   \n",
       "\n",
       "  Accept-Language    Pragma Cache-Control Connection  Body Length URL Length  \\\n",
       "0              en  no-cache           NaN      close            0         39   \n",
       "1              en  no-cache           NaN      close            0        117   \n",
       "2              en  no-cache           NaN      close            0         48   \n",
       "3              en  no-cache           NaN      close            0        116   \n",
       "4              en  no-cache           NaN      close            0         52   \n",
       "\n",
       "   Special Characters in URL  Special Characters in Query Params  \n",
       "0                      False                               False  \n",
       "1                      False                               False  \n",
       "2                      False                               False  \n",
       "3                      False                               False  \n",
       "4                      False                               False  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_file_parsed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=normal_file_parsed.shape[1]\n",
    "n_samples =normal_file_parsed.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of features: 22\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Method                                    0\n",
       "Full URL                                  0\n",
       "URL Path                                  0\n",
       "Query Params                          28000\n",
       "Query Params Length                       0\n",
       "Number of Query Params                    0\n",
       "Body Params                           36000\n",
       "Content-Length                        28000\n",
       "Content-Type                          28000\n",
       "User-Agent                                0\n",
       "Host                                      0\n",
       "Accept                                    0\n",
       "Accept-Encoding                           0\n",
       "Accept-Charset                            0\n",
       "Accept-Language                           0\n",
       "Pragma                                    0\n",
       "Cache-Control                         36000\n",
       "Connection                                0\n",
       "Body Length                               0\n",
       "URL Length                                0\n",
       "Special Characters in URL                 0\n",
       "Special Characters in Query Params        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'number of features: {n_features}')\n",
    "missing_values_count = normal_file_parsed.isnull().sum()\n",
    "missing_values_count[0:n_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage missing: 12.12 %\n"
     ]
    }
   ],
   "source": [
    "total_cells = np.prod(normal_file_parsed.shape)\n",
    "total_missing = missing_values_count.sum()\n",
    "percent_missing = (total_missing/total_cells) * 100\n",
    "print('percentage missing:',(f'{percent_missing:.2f}') ,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values for Method: 2\n",
      "Number of unique values for Full URL: 4840\n",
      "Number of unique values for URL Path: 28\n",
      "Number of unique values for Query Params: 4812\n",
      "Number of unique values for Query Params Length: 117\n",
      "Number of unique values for Number of Query Params: 5\n",
      "Number of unique values for Body Params: 0\n",
      "Number of unique values for Content-Length: 116\n",
      "Number of unique values for Content-Type: 1\n",
      "Number of unique values for User-Agent: 1\n",
      "Number of unique values for Host: 1\n",
      "Number of unique values for Accept: 1\n",
      "Number of unique values for Accept-Encoding: 1\n",
      "Number of unique values for Accept-Charset: 1\n",
      "Number of unique values for Accept-Language: 1\n",
      "Number of unique values for Pragma: 1\n",
      "Number of unique values for Cache-Control: 0\n",
      "Number of unique values for Connection: 1\n",
      "Number of unique values for Body Length: 1\n",
      "Number of unique values for URL Length: 124\n",
      "Number of unique values for Special Characters in URL: 1\n",
      "Number of unique values for Special Characters in Query Params: 2\n"
     ]
    }
   ],
   "source": [
    "for feature in normal_file_parsed.columns:\n",
    "    if feature in normal_file_parsed.columns:\n",
    "        unique_count = normal_file_parsed[feature].nunique()\n",
    "        print(f\"Number of unique values for {feature}: {unique_count}\")\n",
    "    else:\n",
    "        print(f\"Column '{feature}' does not exist in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized data has been saved to anomalous_tokenized_data2.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def tokenize_field(field_value):\n",
    "    # Tokenize the field value by splitting on non-alphanumeric characters and whitespace\n",
    "    tokens = re.split(r'\\W+|_', field_value)\n",
    "    return [token for token in tokens if token]  # Remove empty tokens\n",
    "\n",
    "def tokenize_http_request_row(row):\n",
    "    tokens = []\n",
    "    for field in row:\n",
    "        tokens.extend(tokenize_field(field))\n",
    "    return tokens\n",
    "\n",
    "def process_csv(input_file_path, output_file_path):\n",
    "    all_tokens = []\n",
    "\n",
    "    # Read the CSV file\n",
    "    with open(input_file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        headers = next(reader)  # Skip the header row if present\n",
    "\n",
    "        # Tokenize each row in the CSV file\n",
    "        for row in reader:\n",
    "            row_tokens = tokenize_http_request_row(row)\n",
    "            all_tokens.append(row_tokens)\n",
    "\n",
    "    # Write tokenized data to a new CSV file\n",
    "    with open(output_file_path, mode='w', encoding='utf-8', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for tokens in all_tokens:\n",
    "            writer.writerow(tokens)\n",
    "\n",
    "def main():\n",
    "    input_file_path = 'anomalous_parsed_data2.csv'  # Replace with your CSV file path\n",
    "    output_file_path = 'anomalous_tokenized_data2.csv'  # Path to save the tokenized data\n",
    "    process_csv(input_file_path, output_file_path)\n",
    "    print(f\"Tokenized data has been saved to {output_file_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization Using TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading CSV file: read_csv() got an unexpected keyword argument 'error_bad_lines'\n",
      "Failed to read tokenized CSV file.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def read_tokenized_csv(file_path):\n",
    "    # Attempt to read the CSV file with error handling\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, header=None, error_bad_lines=False, quoting=3)  # quoting=3 to handle any unexpected quoting\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading CSV file: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Combine tokens from each row into a single string\n",
    "    df_combined = df.apply(lambda row: ' '.join(row.dropna().astype(str)), axis=1)\n",
    "    return df_combined\n",
    "\n",
    "def perform_tfidf_transformation(text_data):\n",
    "    # Initialize the TF-IDF Vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    # Fit and transform the data\n",
    "    tfidf_matrix = vectorizer.fit_transform(text_data)\n",
    "    return tfidf_matrix, vectorizer.get_feature_names_out()\n",
    "\n",
    "def save_tfidf_to_csv(tfidf_matrix, feature_names, output_file_path):\n",
    "    # Convert the TF-IDF matrix to a DataFrame\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "    # Save the DataFrame to a CSV file\n",
    "    tfidf_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "def main():\n",
    "    tokenized_file_path = 'anomalous_tokenized_data2.csv'  # Replace with your tokenized CSV file path\n",
    "    tfidf_output_file_path = 'anomalous_vectorised_data2'  # Path to save the TF-IDF matrix\n",
    "\n",
    "    # Step 1: Read the tokenized CSV file\n",
    "    text_data = read_tokenized_csv(tokenized_file_path)\n",
    "    if text_data is None:\n",
    "        print(\"Failed to read tokenized CSV file.\")\n",
    "        return\n",
    "\n",
    "    # Step 2: Perform TF-IDF transformation\n",
    "    tfidf_matrix, feature_names = perform_tfidf_transformation(text_data)\n",
    "\n",
    "    # Step 3: Save the TF-IDF matrix to a CSV file\n",
    "    save_tfidf_to_csv(tfidf_matrix, feature_names, tfidf_output_file_path)\n",
    "\n",
    "    print(f\"TF-IDF matrix has been saved to {tfidf_output_file_path}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection using Information Gain Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m y_encoded \u001b[38;5;241m=\u001b[39m LabelEncoder()\u001b[38;5;241m.\u001b[39mfit_transform(y)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Calculate Information Gain\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m mi \u001b[38;5;241m=\u001b[39m \u001b[43mmutual_info_classif\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiscrete_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Create a DataFrame to hold feature names and their Information Gain values\u001b[39;00m\n\u001b[0;32m     24\u001b[0m feature_info_gain \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m'\u001b[39m: X_encoded\u001b[38;5;241m.\u001b[39mcolumns,\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInformation Gain\u001b[39m\u001b[38;5;124m'\u001b[39m: mi\n\u001b[0;32m     27\u001b[0m })\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:571\u001b[0m, in \u001b[0;36mmutual_info_classif\u001b[1;34m(X, y, discrete_features, n_neighbors, copy, random_state, n_jobs)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Estimate mutual information for a discrete target variable.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m \n\u001b[0;32m    477\u001b[0m \u001b[38;5;124;03mMutual information (MI) [1]_ between two random variables is a non-negative\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;124;03m       0.     , 0.     , 0.     , 0.      , 0.        ])\u001b[39;00m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m check_classification_targets(y)\n\u001b[1;32m--> 571\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_estimate_mi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscrete_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscrete_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscrete_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    578\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:271\u001b[0m, in \u001b[0;36m_estimate_mi\u001b[1;34m(X, y, discrete_features, discrete_target, n_neighbors, copy, random_state, n_jobs)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_estimate_mi\u001b[39m(\n\u001b[0;32m    203\u001b[0m     X,\n\u001b[0;32m    204\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    211\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    212\u001b[0m ):\n\u001b[0;32m    213\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Estimate mutual information between the features and the target.\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \n\u001b[0;32m    215\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;124;03m           Data Sets\". PLoS ONE 9(2), 2014.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m     X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdiscrete_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     n_samples, n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(discrete_features, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m)):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:1301\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1296\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1299\u001b[0m     )\n\u001b[1;32m-> 1301\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1320\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('All.csv')\n",
    "\n",
    "# Identify the target column (assuming it's the last column)\n",
    "target_column = data.columns[-1]\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data[target_column]  # Last column as the target\n",
    "\n",
    "# Encode categorical features and target variable if necessary\n",
    "X_encoded = X.copy()\n",
    "for column in X_encoded.select_dtypes(include=['object']).columns:\n",
    "    X_encoded[column] = LabelEncoder().fit_transform(X_encoded[column])\n",
    "\n",
    "y_encoded = LabelEncoder().fit_transform(y)\n",
    "\n",
    "# Calculate Information Gain\n",
    "mi = mutual_info_classif(X_encoded, y_encoded, discrete_features=True)\n",
    "\n",
    "# Create a DataFrame to hold feature names and their Information Gain values\n",
    "feature_info_gain = pd.DataFrame({\n",
    "    'Feature': X_encoded.columns,\n",
    "    'Information Gain': mi\n",
    "})\n",
    "\n",
    "# Filter features with Information Gain > 0\n",
    "selected_features = feature_info_gain[feature_info_gain['Information Gain'] > 0]\n",
    "\n",
    "# Print selected features\n",
    "print(\"Features with Information Gain > 0:\")\n",
    "print(selected_features)\n",
    "\n",
    "# Save the selected features to a new CSV file if needed\n",
    "selected_features.to_csv('selected_features.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Missing Values with a Specific Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with Information Gain > 0:\n",
      "                  Feature  Information Gain\n",
      "0             Querylength          0.335475\n",
      "1      domain_token_count          0.297508\n",
      "2        path_token_count          0.187484\n",
      "3       avgdomaintokenlen          0.454635\n",
      "4      longdomaintokenlen          0.200259\n",
      "..                    ...               ...\n",
      "74         Entropy_Domain          1.120500\n",
      "75  Entropy_DirectoryName          0.860996\n",
      "76       Entropy_Filename          1.117054\n",
      "77      Entropy_Extension          0.736039\n",
      "78      Entropy_Afterpath          0.693892\n",
      "\n",
      "[78 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('All.csv')\n",
    "\n",
    "# Identify the target column (assuming it's the last column)\n",
    "target_column = data.columns[-1]\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data[target_column]  # Last column as the target\n",
    "\n",
    "# Handle missing values by filling or dropping\n",
    "X_filled = X.fillna(X.mean())  # Fill numerical columns with the mean\n",
    "for column in X_filled.select_dtypes(include=['object']).columns:\n",
    "    X_filled[column] = X_filled[column].fillna(X_filled[column].mode()[0])\n",
    "\n",
    "# Handle target variable\n",
    "y_filled = y.fillna(y.mode()[0])\n",
    "\n",
    "# Replace infinity values with NaN, then fill them\n",
    "X_filled.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_filled = X_filled.fillna(X_filled.mean())  # Refill any NaN values after replacement\n",
    "\n",
    "# Convert any remaining large values to a more manageable range if necessary\n",
    "# For example, you could clip values to a reasonable range\n",
    "# X_filled = X_filled.clip(-1e10, 1e10)  # Adjust these limits as needed\n",
    "\n",
    "# Encode categorical features and target variable if necessary\n",
    "X_encoded = X_filled.copy()\n",
    "for column in X_encoded.select_dtypes(include=['object']).columns:\n",
    "    X_encoded[column] = LabelEncoder().fit_transform(X_encoded[column])\n",
    "\n",
    "y_encoded = LabelEncoder().fit_transform(y_filled)\n",
    "\n",
    "# Calculate Information Gain\n",
    "mi = mutual_info_classif(X_encoded, y_encoded, discrete_features=True)\n",
    "\n",
    "# Create a DataFrame to hold feature names and their Information Gain values\n",
    "feature_info_gain = pd.DataFrame({\n",
    "    'Feature': X_encoded.columns,\n",
    "    'Information Gain': mi\n",
    "})\n",
    "\n",
    "# Filter features with Information Gain > 0\n",
    "selected_features = feature_info_gain[feature_info_gain['Information Gain'] > 0]\n",
    "\n",
    "# Print selected features\n",
    "print(\"Features with Information Gain > 0:\")\n",
    "print(selected_features)\n",
    "\n",
    "# Save the selected features to a new CSV file if needed\n",
    "selected_features.to_csv('selected_features.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Rows with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with Information Gain > 0:\n",
      "                  Feature  Information Gain\n",
      "0             Querylength          0.385037\n",
      "1      domain_token_count          0.291890\n",
      "2        path_token_count          0.190394\n",
      "3       avgdomaintokenlen          0.555699\n",
      "4      longdomaintokenlen          0.273381\n",
      "..                    ...               ...\n",
      "74         Entropy_Domain          1.144740\n",
      "75  Entropy_DirectoryName          1.050474\n",
      "76       Entropy_Filename          1.061668\n",
      "77      Entropy_Extension          0.799772\n",
      "78      Entropy_Afterpath          0.657151\n",
      "\n",
      "[78 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n",
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\metrics\\cluster\\_supervised.py:66: UserWarning: Clustering metrics expects discrete values but received continuous values for label, and multiclass values for target\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load your dataset\n",
    "data = pd.read_csv('All.csv')\n",
    "\n",
    "# Identify the target column (assuming it's the last column)\n",
    "target_column = data.columns[-1]\n",
    "X = data.iloc[:, :-1]  # All columns except the last one\n",
    "y = data[target_column]  # Last column as the target\n",
    "\n",
    "# Handle missing values by dropping rows with any NaNs\n",
    "data_cleaned = data.dropna()\n",
    "\n",
    "# Separate the cleaned features and target variable\n",
    "X_cleaned = data_cleaned.iloc[:, :-1]\n",
    "y_cleaned = data_cleaned[target_column]\n",
    "\n",
    "# Encode categorical features and target variable if necessary\n",
    "X_encoded = X_cleaned.copy()\n",
    "for column in X_encoded.select_dtypes(include=['object']).columns:\n",
    "    X_encoded[column] = LabelEncoder().fit_transform(X_encoded[column])\n",
    "\n",
    "y_encoded = LabelEncoder().fit_transform(y_cleaned)\n",
    "\n",
    "# Calculate Information Gain\n",
    "mi = mutual_info_classif(X_encoded, y_encoded, discrete_features=True)\n",
    "\n",
    "# Create a DataFrame to hold feature names and their Information Gain values\n",
    "feature_info_gain = pd.DataFrame({\n",
    "    'Feature': X_encoded.columns,\n",
    "    'Information Gain': mi\n",
    "})\n",
    "\n",
    "# Filter features with Information Gain > 0\n",
    "selected_features = feature_info_gain[feature_info_gain['Information Gain'] > 0]\n",
    "\n",
    "# Print selected features\n",
    "print(\"Features with Information Gain > 0:\")\n",
    "print(selected_features)\n",
    "\n",
    "# Save the selected features to a new CSV file if needed\n",
    "selected_features.to_csv('selected_features2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_exploit='All.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_exploit=pd.read_csv('All.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Querylength</th>\n",
       "      <th>domain_token_count</th>\n",
       "      <th>path_token_count</th>\n",
       "      <th>avgdomaintokenlen</th>\n",
       "      <th>longdomaintokenlen</th>\n",
       "      <th>avgpathtokenlen</th>\n",
       "      <th>tld</th>\n",
       "      <th>charcompvowels</th>\n",
       "      <th>charcompace</th>\n",
       "      <th>ldl_url</th>\n",
       "      <th>...</th>\n",
       "      <th>SymbolCount_FileName</th>\n",
       "      <th>SymbolCount_Extension</th>\n",
       "      <th>SymbolCount_Afterpath</th>\n",
       "      <th>Entropy_URL</th>\n",
       "      <th>Entropy_Domain</th>\n",
       "      <th>Entropy_DirectoryName</th>\n",
       "      <th>Entropy_Filename</th>\n",
       "      <th>Entropy_Extension</th>\n",
       "      <th>Entropy_Afterpath</th>\n",
       "      <th>URL_Type_obf_Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>14</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.726298</td>\n",
       "      <td>0.784493</td>\n",
       "      <td>0.894886</td>\n",
       "      <td>0.850608</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Defacement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>14</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.688635</td>\n",
       "      <td>0.784493</td>\n",
       "      <td>0.814725</td>\n",
       "      <td>0.859793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Defacement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>14</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.695049</td>\n",
       "      <td>0.784493</td>\n",
       "      <td>0.814725</td>\n",
       "      <td>0.801880</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Defacement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>5.5</td>\n",
       "      <td>14</td>\n",
       "      <td>5.500000</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.640130</td>\n",
       "      <td>0.784493</td>\n",
       "      <td>0.814725</td>\n",
       "      <td>0.663210</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Defacement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5.5</td>\n",
       "      <td>14</td>\n",
       "      <td>7.333334</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.681307</td>\n",
       "      <td>0.784493</td>\n",
       "      <td>0.814725</td>\n",
       "      <td>0.804526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>Defacement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Querylength  domain_token_count  path_token_count  avgdomaintokenlen  \\\n",
       "0            0                   4                 5                5.5   \n",
       "1            0                   4                 5                5.5   \n",
       "2            0                   4                 5                5.5   \n",
       "3            0                   4                12                5.5   \n",
       "4            0                   4                 6                5.5   \n",
       "\n",
       "   longdomaintokenlen  avgpathtokenlen  tld  charcompvowels  charcompace  \\\n",
       "0                  14         4.400000    4               8            3   \n",
       "1                  14         6.000000    4              12            4   \n",
       "2                  14         5.800000    4              12            5   \n",
       "3                  14         5.500000    4              32           16   \n",
       "4                  14         7.333334    4              18           11   \n",
       "\n",
       "   ldl_url  ...  SymbolCount_FileName  SymbolCount_Extension  \\\n",
       "0        0  ...                     1                      0   \n",
       "1        0  ...                     0                      0   \n",
       "2        0  ...                     0                      0   \n",
       "3        0  ...                     0                      0   \n",
       "4        0  ...                     0                      0   \n",
       "\n",
       "   SymbolCount_Afterpath  Entropy_URL  Entropy_Domain  Entropy_DirectoryName  \\\n",
       "0                     -1     0.726298        0.784493               0.894886   \n",
       "1                     -1     0.688635        0.784493               0.814725   \n",
       "2                     -1     0.695049        0.784493               0.814725   \n",
       "3                     -1     0.640130        0.784493               0.814725   \n",
       "4                     -1     0.681307        0.784493               0.814725   \n",
       "\n",
       "   Entropy_Filename  Entropy_Extension  Entropy_Afterpath  URL_Type_obf_Type  \n",
       "0          0.850608                NaN               -1.0         Defacement  \n",
       "1          0.859793                0.0               -1.0         Defacement  \n",
       "2          0.801880                0.0               -1.0         Defacement  \n",
       "3          0.663210                0.0               -1.0         Defacement  \n",
       "4          0.804526                0.0               -1.0         Defacement  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_exploit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
